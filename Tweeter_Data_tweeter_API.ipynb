{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVJWbbaj0Vqg",
        "outputId": "03a45b21-8f7a-4ac3-ac59-cb8fc785ec86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from snscrape) (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from snscrape) (5.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from snscrape) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from snscrape) (3.18.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Downloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/74.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snscrape\n",
            "Successfully installed snscrape-0.7.0.20230622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UL84jHh8zzYa",
        "outputId": "35edce02-1a98-4737-aa54-649c3f43e1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  IMG seeking on-site research position in Neurology/Neuroscience (J1 visa needed).\n",
            "Skilled in Python, R, ML, &amp; clinical research.\n",
            "USMLE Step 1 &amp; 2CK prep doneâ€”exams pending due to travel costs.\n",
            "ğŸ“© CV, LORs, &amp; cover letter available.\n",
            "#IMG #OpenToWork #Neuroscience #J1Visa\n",
            "Pythonã€ITãƒ‘ã‚¹ãƒãƒ¼ãƒˆã®æ“¬ä¼¼è¨€èªã‚’æ›¸ã„ã¦ã„ãŸã®ã§ã€ã‚„ã£ã¦ã¿ã‚ˆã£ã¦ãªã£ãŸã€‚Pythonã®è³‡æ ¼ã‚’å–å¾—ã—ãŸã‚‰ã€Pythonã§æ¥­å‹™ã‚’åŠ¹ç‡åŒ–ï¼ https://t.co/MDMtlDBdCc\n",
            "RT @GrowAIHub: PYTHON is difficult to learn, but not anymore!\n",
            "\n",
            "Introducing \"The Ultimate Python ebook \"PDF.\n",
            "\n",
            "You will get:\n",
            "\n",
            "â€¢ 74+ pages cheâ€¦\n",
            "PYTHON is difficult to learn, but not anymore!\n",
            "\n",
            "Introducing \"The Ultimate Python ebook \"PDF.\n",
            "\n",
            "You will get:\n",
            "\n",
            "â€¢ 74+ pages cheatsheet\n",
            "â€¢ Save 100+ hours on research\n",
            "\n",
            "And for 48 hrs, it's 100% FREE!\n",
            "\n",
            "To get it, just:\n",
            "\n",
            "1. Like &amp; RT\n",
            "2. Reply \"SEND\"\n",
            "3. Follow @GrowAIHub [MUST] https://t.co/u8EMFY4zF1\n",
            "RT @yugen_matuni: éå¸¸ã«èª­ã¿ã¥ã‚‰ã„ç”»åƒã‹ã‚‰ã®VLMã€‚\n",
            "o3ã¯æµçŸ³ã§ã€éƒ¨åˆ†éƒ¨åˆ†ã‚’æ‹¡å¤§ã—ãªãŒã‚‰ã®VLMãªã®ã§ã€ç²¾åº¦ãŒé«˜ã„ã€‚\n",
            "\n",
            "ä¸€æ–¹ã§Grok4ã®åˆ©ç”¨å¯èƒ½ãªPythonã®ç¨®é¡ãŒå°‘ãªã„ã®ã§ã€o3ã®ã‚ˆã†ãªåå¾©è§£æã¯ã§ããšç²¾åº¦ã¯ãƒ€ãƒ¡ãƒ€ãƒ¡ã€‚\n",
            "\n",
            "Grok4ã¯æ—¥æœ¬èªã§ã¯ã‚ã¾ã‚Šç›®â€¦\n",
            "ãã†ã„ãˆã°ã¾ã æ¥­å‹™ã‚¢ã‚µã‚¤ãƒ³ã•ã‚Œã¦ãªã„ã‹ã‚‰ã²ãŸã™ã‚‰ç¤¾å†…è³‡æ–™èª­ã‚“ã§ãŠå‹‰å¼·ä¸­ï¼ˆè‡ªåˆ†ã§æ¼ã‚Šã«è¡Œã£ã¦ã‚‹ï¼‰\n",
            "\n",
            "ã‚ã¾ã‚Šã«ã‚‚ï¼ˆå…·ä½“çš„ãªæ¥­å‹™ãŒãªã„ã®ã§ï¼‰é€€å±ˆãªã®ã§åˆé–“åˆé–“ã§ã¡ã‚‡ã“ã¡ã‚‡ã“pythonã§ä¾¿åˆ©ãƒ„ãƒ¼ãƒ«ä½œã£ã¦ãŸã€‚ä»•äº‹ç€æ‰‹ã—ã¦ã‹ã‚‰ä½œæ¥­è² æ‹…æ¸›ã‚‹ã‚ˆã†ã«ã€‚\n",
            "@PublicAI_ Finally, an AI revolution where I donâ€™t have to pretend I know Python. #PublicAI for the win! ğŸš€ https://t.co/FClU2iKXci\n",
            "@PublicAI_ Finally, an AI revolution where I donâ€™t have to pretend I know Python. #PublicAI letâ€™s goooo! ğŸš€ https://t.co/0b2KAqMYHA\n",
            "RT @peaq: import peaq\n",
            "build(AI for Machine Economy)\n",
            "\n",
            "You guessed itâ€”peaq just got its Python SDK\n",
            "\n",
            "ğŸ”— https://t.co/FqHNCF0ND3\n",
            "\n",
            "Connect to peaâ€¦\n",
            "RT @pyconafrica: â° ONLY 11 DAYS Left!\n",
            "Need support to attend #PyConAfrica2025?\n",
            "Apply for an Opportunity Grant by July 19 !\n",
            "ğŸ‘‰ Find out moreâ€¦\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "\n",
        "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAPr92wEAAAAAvkpvGWVd6j0AvIE8WOCjoNn30FI%3D94dvPW2kKFXRAsSbggUNGhCnCg1FulM4N3ne07TtqCmAA64iZ5')\n",
        "\n",
        "# Example search\n",
        "response = client.search_recent_tweets(\"Python\", max_results=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in response.data:\n",
        "    print(tweet.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KerWkxb0RBQ",
        "outputId": "426570a5-5be5-4623-fe8f-ee597be10c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  IMG seeking on-site research position in Neurology/Neuroscience (J1 visa needed).\n",
            "Skilled in Python, R, ML, &amp; clinical research.\n",
            "USMLE Step 1 &amp; 2CK prep doneâ€”exams pending due to travel costs.\n",
            "ğŸ“© CV, LORs, &amp; cover letter available.\n",
            "#IMG #OpenToWork #Neuroscience #J1Visa\n",
            "Pythonã€ITãƒ‘ã‚¹ãƒãƒ¼ãƒˆã®æ“¬ä¼¼è¨€èªã‚’æ›¸ã„ã¦ã„ãŸã®ã§ã€ã‚„ã£ã¦ã¿ã‚ˆã£ã¦ãªã£ãŸã€‚Pythonã®è³‡æ ¼ã‚’å–å¾—ã—ãŸã‚‰ã€Pythonã§æ¥­å‹™ã‚’åŠ¹ç‡åŒ–ï¼ https://t.co/MDMtlDBdCc\n",
            "RT @GrowAIHub: PYTHON is difficult to learn, but not anymore!\n",
            "\n",
            "Introducing \"The Ultimate Python ebook \"PDF.\n",
            "\n",
            "You will get:\n",
            "\n",
            "â€¢ 74+ pages cheâ€¦\n",
            "PYTHON is difficult to learn, but not anymore!\n",
            "\n",
            "Introducing \"The Ultimate Python ebook \"PDF.\n",
            "\n",
            "You will get:\n",
            "\n",
            "â€¢ 74+ pages cheatsheet\n",
            "â€¢ Save 100+ hours on research\n",
            "\n",
            "And for 48 hrs, it's 100% FREE!\n",
            "\n",
            "To get it, just:\n",
            "\n",
            "1. Like &amp; RT\n",
            "2. Reply \"SEND\"\n",
            "3. Follow @GrowAIHub [MUST] https://t.co/u8EMFY4zF1\n",
            "RT @yugen_matuni: éå¸¸ã«èª­ã¿ã¥ã‚‰ã„ç”»åƒã‹ã‚‰ã®VLMã€‚\n",
            "o3ã¯æµçŸ³ã§ã€éƒ¨åˆ†éƒ¨åˆ†ã‚’æ‹¡å¤§ã—ãªãŒã‚‰ã®VLMãªã®ã§ã€ç²¾åº¦ãŒé«˜ã„ã€‚\n",
            "\n",
            "ä¸€æ–¹ã§Grok4ã®åˆ©ç”¨å¯èƒ½ãªPythonã®ç¨®é¡ãŒå°‘ãªã„ã®ã§ã€o3ã®ã‚ˆã†ãªåå¾©è§£æã¯ã§ããšç²¾åº¦ã¯ãƒ€ãƒ¡ãƒ€ãƒ¡ã€‚\n",
            "\n",
            "Grok4ã¯æ—¥æœ¬èªã§ã¯ã‚ã¾ã‚Šç›®â€¦\n",
            "ãã†ã„ãˆã°ã¾ã æ¥­å‹™ã‚¢ã‚µã‚¤ãƒ³ã•ã‚Œã¦ãªã„ã‹ã‚‰ã²ãŸã™ã‚‰ç¤¾å†…è³‡æ–™èª­ã‚“ã§ãŠå‹‰å¼·ä¸­ï¼ˆè‡ªåˆ†ã§æ¼ã‚Šã«è¡Œã£ã¦ã‚‹ï¼‰\n",
            "\n",
            "ã‚ã¾ã‚Šã«ã‚‚ï¼ˆå…·ä½“çš„ãªæ¥­å‹™ãŒãªã„ã®ã§ï¼‰é€€å±ˆãªã®ã§åˆé–“åˆé–“ã§ã¡ã‚‡ã“ã¡ã‚‡ã“pythonã§ä¾¿åˆ©ãƒ„ãƒ¼ãƒ«ä½œã£ã¦ãŸã€‚ä»•äº‹ç€æ‰‹ã—ã¦ã‹ã‚‰ä½œæ¥­è² æ‹…æ¸›ã‚‹ã‚ˆã†ã«ã€‚\n",
            "@PublicAI_ Finally, an AI revolution where I donâ€™t have to pretend I know Python. #PublicAI for the win! ğŸš€ https://t.co/FClU2iKXci\n",
            "@PublicAI_ Finally, an AI revolution where I donâ€™t have to pretend I know Python. #PublicAI letâ€™s goooo! ğŸš€ https://t.co/0b2KAqMYHA\n",
            "RT @peaq: import peaq\n",
            "build(AI for Machine Economy)\n",
            "\n",
            "You guessed itâ€”peaq just got its Python SDK\n",
            "\n",
            "ğŸ”— https://t.co/FqHNCF0ND3\n",
            "\n",
            "Connect to peaâ€¦\n",
            "RT @pyconafrica: â° ONLY 11 DAYS Left!\n",
            "Need support to attend #PyConAfrica2025?\n",
            "Apply for an Opportunity Grant by July 19 !\n",
            "ğŸ‘‰ Find out moreâ€¦\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Twitter client\n",
        "client = tweepy.Client(bearer_token='YOUR_BEARER_TOKEN')\n",
        "\n",
        "# Search query for Kannada religious content\n",
        "query = (\"(à²¹à²¿à²‚à²¦à³‚ OR à²¹à²°à²¿ OR à²¹à²°à²¿à²¹à²° OR à²µà²¿à²·à³à²£à³ OR à²¶à²¿à²µ OR à²—à²£à³‡à²¶ OR à²²à²•à³à²·à³à²®à³€ OR à²¸à²°à²¸à³à²µà²¤à³€ OR à²¦à³à²°à³à²—à²¾ OR \"\n",
        "         \"à²•à³ƒà²·à³à²£ OR à²°à²¾à²® OR à²¹à²¨à³à²®à²‚à²¤ OR à²µà³€à²°à²­à²¦à³à²° OR à²¶à²¨à²¿ OR à²¯à³à²—à²¾à²¦à²¿ OR à²¦à³€à²ªà²¾à²µà²³à²¿ OR \"\n",
        "         \"à²—à²£à³‡à²¶ à²šà²¤à³à²°à³à²¥à²¿ OR à²¨à²µà²°à²¾à²¤à³à²°à²¿ OR à²¶à²¿à²µà²°à²¾à²¤à³à²°à²¿) lang:kn -is:retweet\")\n",
        "\n",
        "# Define all fields to collect\n",
        "tweet_fields = [\n",
        "    'author_id', 'created_at', 'public_metrics', 'source',\n",
        "    'context_annotations', 'entities', 'geo', 'lang'\n",
        "]\n",
        "\n",
        "expansions = ['author_id', 'geo.place_id']\n",
        "\n",
        "# Collect tweets\n",
        "tweets = []\n",
        "users = []\n",
        "\n",
        "try:\n",
        "    response = client.search_recent_tweets(\n",
        "        query=query,\n",
        "        max_results=1,  # Max per request\n",
        "        tweet_fields=tweet_fields,\n",
        "        expansions=expansions,\n",
        "        user_fields=['username', 'name', 'location', 'verified', 'description']\n",
        "    )\n",
        "\n",
        "    # Process tweets\n",
        "    if response.data:\n",
        "        for tweet in response.data:\n",
        "            tweet_data = {\n",
        "                'tweet_id': tweet.id,\n",
        "                'text': tweet.text,\n",
        "                'created_at': tweet.created_at,\n",
        "                'author_id': tweet.author_id,\n",
        "                'source': tweet.source,\n",
        "                'language': tweet.lang,\n",
        "                'like_count': tweet.public_metrics['like_count'],\n",
        "                'retweet_count': tweet.public_metrics['retweet_count'],\n",
        "                'reply_count': tweet.public_metrics['reply_count'],\n",
        "                'quote_count': tweet.public_metrics['quote_count'],\n",
        "                'hashtags': [],\n",
        "                'mentions': [],\n",
        "                'urls': []\n",
        "            }\n",
        "\n",
        "            # Extract entities if available\n",
        "            if tweet.entities:\n",
        "                if 'hashtags' in tweet.entities:\n",
        "                    tweet_data['hashtags'] = [tag['tag'] for tag in tweet.entities['hashtags']]\n",
        "                if 'mentions' in tweet.entities:\n",
        "                    tweet_data['mentions'] = [mention['username'] for mention in tweet.entities['mentions']]\n",
        "                if 'urls' in tweet.entities:\n",
        "                    tweet_data['urls'] = [url['expanded_url'] for url in tweet.entities['urls'] if 'expanded_url' in url]\n",
        "\n",
        "            tweets.append(tweet_data)\n",
        "\n",
        "        # Process user information\n",
        "        if response.includes and 'users' in response.includes:\n",
        "            for user in response.includes['users']:\n",
        "                user_data = {\n",
        "                    'author_id': user.id,\n",
        "                    'username': user.username,\n",
        "                    'name': user.name,\n",
        "                    'verified': user.verified,\n",
        "                    'description': user.description,\n",
        "                    'location': user.location\n",
        "                }\n",
        "                users.append(user_data)\n",
        "\n",
        "    # Create DataFrames\n",
        "    tweets_df = pd.DataFrame(tweets)\n",
        "    users_df = pd.DataFrame(users)\n",
        "\n",
        "    # Merge tweet and user data\n",
        "    full_df = pd.merge(tweets_df, users_df, on='author_id', how='left')\n",
        "\n",
        "    # Save to CSV with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"kannada_religious_tweets_{timestamp}.csv\"\n",
        "    full_df.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"Successfully collected {len(tweets)} tweets. Saved to {filename}\")\n",
        "    print(f\"Dataset columns: {full_df.columns.tolist()}\")\n",
        "\n",
        "except tweepy.TweepyException as e:\n",
        "    print(f\"Error fetching tweets: {e}\")"
      ],
      "metadata": {
        "id": "tbAN8u5C4iqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}