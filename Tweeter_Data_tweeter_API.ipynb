{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVJWbbaj0Vqg",
        "outputId": "03a45b21-8f7a-4ac3-ac59-cb8fc785ec86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from snscrape) (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from snscrape) (5.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from snscrape) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from snscrape) (3.18.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Downloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snscrape\n",
            "Successfully installed snscrape-0.7.0.20230622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UL84jHh8zzYa",
        "outputId": "35edce02-1a98-4737-aa54-649c3f43e1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 IMG seeking on-site research position in Neurology/Neuroscience (J1 visa needed).\n",
            "Skilled in Python, R, ML, &amp; clinical research.\n",
            "USMLE Step 1 &amp; 2CK prep done—exams pending due to travel costs.\n",
            "📩 CV, LORs, &amp; cover letter available.\n",
            "#IMG #OpenToWork #Neuroscience #J1Visa\n",
            "Python、ITパスポートの擬似言語を書いていたので、やってみよってなった。Pythonの資格を取得したら、Pythonで業務を効率化！ https://t.co/MDMtlDBdCc\n",
            "RT @GrowAIHub: PYTHON is difficult to learn, but not anymore!\n",
            "\n",
            "Introducing \"The Ultimate Python ebook \"PDF.\n",
            "\n",
            "You will get:\n",
            "\n",
            "• 74+ pages che…\n",
            "PYTHON is difficult to learn, but not anymore!\n",
            "\n",
            "Introducing \"The Ultimate Python ebook \"PDF.\n",
            "\n",
            "You will get:\n",
            "\n",
            "• 74+ pages cheatsheet\n",
            "• Save 100+ hours on research\n",
            "\n",
            "And for 48 hrs, it's 100% FREE!\n",
            "\n",
            "To get it, just:\n",
            "\n",
            "1. Like &amp; RT\n",
            "2. Reply \"SEND\"\n",
            "3. Follow @GrowAIHub [MUST] https://t.co/u8EMFY4zF1\n",
            "RT @yugen_matuni: 非常に読みづらい画像からのVLM。\n",
            "o3は流石で、部分部分を拡大しながらのVLMなので、精度が高い。\n",
            "\n",
            "一方でGrok4の利用可能なPythonの種類が少ないので、o3のような反復解析はできず精度はダメダメ。\n",
            "\n",
            "Grok4は日本語ではあまり目…\n",
            "そういえばまだ業務アサインされてないからひたすら社内資料読んでお勉強中（自分で漁りに行ってる）\n",
            "\n",
            "あまりにも（具体的な業務がないので）退屈なので合間合間でちょこちょこpythonで便利ツール作ってた。仕事着手してから作業負担減るように。\n",
            "@PublicAI_ Finally, an AI revolution where I don’t have to pretend I know Python. #PublicAI for the win! 🚀 https://t.co/FClU2iKXci\n",
            "@PublicAI_ Finally, an AI revolution where I don’t have to pretend I know Python. #PublicAI let’s goooo! 🚀 https://t.co/0b2KAqMYHA\n",
            "RT @peaq: import peaq\n",
            "build(AI for Machine Economy)\n",
            "\n",
            "You guessed it—peaq just got its Python SDK\n",
            "\n",
            "🔗 https://t.co/FqHNCF0ND3\n",
            "\n",
            "Connect to pea…\n",
            "RT @pyconafrica: ⏰ ONLY 11 DAYS Left!\n",
            "Need support to attend #PyConAfrica2025?\n",
            "Apply for an Opportunity Grant by July 19 !\n",
            "👉 Find out more…\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "\n",
        "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAPr92wEAAAAAvkpvGWVd6j0AvIE8WOCjoNn30FI%3D94dvPW2kKFXRAsSbggUNGhCnCg1FulM4N3ne07TtqCmAA64iZ5')\n",
        "\n",
        "# Example search\n",
        "response = client.search_recent_tweets(\"Python\", max_results=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in response.data:\n",
        "    print(tweet.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KerWkxb0RBQ",
        "outputId": "426570a5-5be5-4623-fe8f-ee597be10c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 IMG seeking on-site research position in Neurology/Neuroscience (J1 visa needed).\n",
            "Skilled in Python, R, ML, &amp; clinical research.\n",
            "USMLE Step 1 &amp; 2CK prep done—exams pending due to travel costs.\n",
            "📩 CV, LORs, &amp; cover letter available.\n",
            "#IMG #OpenToWork #Neuroscience #J1Visa\n",
            "Python、ITパスポートの擬似言語を書いていたので、やってみよってなった。Pythonの資格を取得したら、Pythonで業務を効率化！ https://t.co/MDMtlDBdCc\n",
            "RT @GrowAIHub: PYTHON is difficult to learn, but not anymore!\n",
            "\n",
            "Introducing \"The Ultimate Python ebook \"PDF.\n",
            "\n",
            "You will get:\n",
            "\n",
            "• 74+ pages che…\n",
            "PYTHON is difficult to learn, but not anymore!\n",
            "\n",
            "Introducing \"The Ultimate Python ebook \"PDF.\n",
            "\n",
            "You will get:\n",
            "\n",
            "• 74+ pages cheatsheet\n",
            "• Save 100+ hours on research\n",
            "\n",
            "And for 48 hrs, it's 100% FREE!\n",
            "\n",
            "To get it, just:\n",
            "\n",
            "1. Like &amp; RT\n",
            "2. Reply \"SEND\"\n",
            "3. Follow @GrowAIHub [MUST] https://t.co/u8EMFY4zF1\n",
            "RT @yugen_matuni: 非常に読みづらい画像からのVLM。\n",
            "o3は流石で、部分部分を拡大しながらのVLMなので、精度が高い。\n",
            "\n",
            "一方でGrok4の利用可能なPythonの種類が少ないので、o3のような反復解析はできず精度はダメダメ。\n",
            "\n",
            "Grok4は日本語ではあまり目…\n",
            "そういえばまだ業務アサインされてないからひたすら社内資料読んでお勉強中（自分で漁りに行ってる）\n",
            "\n",
            "あまりにも（具体的な業務がないので）退屈なので合間合間でちょこちょこpythonで便利ツール作ってた。仕事着手してから作業負担減るように。\n",
            "@PublicAI_ Finally, an AI revolution where I don’t have to pretend I know Python. #PublicAI for the win! 🚀 https://t.co/FClU2iKXci\n",
            "@PublicAI_ Finally, an AI revolution where I don’t have to pretend I know Python. #PublicAI let’s goooo! 🚀 https://t.co/0b2KAqMYHA\n",
            "RT @peaq: import peaq\n",
            "build(AI for Machine Economy)\n",
            "\n",
            "You guessed it—peaq just got its Python SDK\n",
            "\n",
            "🔗 https://t.co/FqHNCF0ND3\n",
            "\n",
            "Connect to pea…\n",
            "RT @pyconafrica: ⏰ ONLY 11 DAYS Left!\n",
            "Need support to attend #PyConAfrica2025?\n",
            "Apply for an Opportunity Grant by July 19 !\n",
            "👉 Find out more…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Twitter client\n",
        "client = tweepy.Client(bearer_token='YOUR_BEARER_TOKEN')\n",
        "\n",
        "# Search query for Kannada religious content\n",
        "query = (\"(ಹಿಂದೂ OR ಹರಿ OR ಹರಿಹರ OR ವಿಷ್ಣು OR ಶಿವ OR ಗಣೇಶ OR ಲಕ್ಷ್ಮೀ OR ಸರಸ್ವತೀ OR ದುರ್ಗಾ OR \"\n",
        "         \"ಕೃಷ್ಣ OR ರಾಮ OR ಹನುಮಂತ OR ವೀರಭದ್ರ OR ಶನಿ OR ಯುಗಾದಿ OR ದೀಪಾವಳಿ OR \"\n",
        "         \"ಗಣೇಶ ಚತುರ್ಥಿ OR ನವರಾತ್ರಿ OR ಶಿವರಾತ್ರಿ) lang:kn -is:retweet\")\n",
        "\n",
        "# Define all fields to collect\n",
        "tweet_fields = [\n",
        "    'author_id', 'created_at', 'public_metrics', 'source',\n",
        "    'context_annotations', 'entities', 'geo', 'lang'\n",
        "]\n",
        "\n",
        "expansions = ['author_id', 'geo.place_id']\n",
        "\n",
        "# Collect tweets\n",
        "tweets = []\n",
        "users = []\n",
        "\n",
        "try:\n",
        "    response = client.search_recent_tweets(\n",
        "        query=query,\n",
        "        max_results=1,  # Max per request\n",
        "        tweet_fields=tweet_fields,\n",
        "        expansions=expansions,\n",
        "        user_fields=['username', 'name', 'location', 'verified', 'description']\n",
        "    )\n",
        "\n",
        "    # Process tweets\n",
        "    if response.data:\n",
        "        for tweet in response.data:\n",
        "            tweet_data = {\n",
        "                'tweet_id': tweet.id,\n",
        "                'text': tweet.text,\n",
        "                'created_at': tweet.created_at,\n",
        "                'author_id': tweet.author_id,\n",
        "                'source': tweet.source,\n",
        "                'language': tweet.lang,\n",
        "                'like_count': tweet.public_metrics['like_count'],\n",
        "                'retweet_count': tweet.public_metrics['retweet_count'],\n",
        "                'reply_count': tweet.public_metrics['reply_count'],\n",
        "                'quote_count': tweet.public_metrics['quote_count'],\n",
        "                'hashtags': [],\n",
        "                'mentions': [],\n",
        "                'urls': []\n",
        "            }\n",
        "\n",
        "            # Extract entities if available\n",
        "            if tweet.entities:\n",
        "                if 'hashtags' in tweet.entities:\n",
        "                    tweet_data['hashtags'] = [tag['tag'] for tag in tweet.entities['hashtags']]\n",
        "                if 'mentions' in tweet.entities:\n",
        "                    tweet_data['mentions'] = [mention['username'] for mention in tweet.entities['mentions']]\n",
        "                if 'urls' in tweet.entities:\n",
        "                    tweet_data['urls'] = [url['expanded_url'] for url in tweet.entities['urls'] if 'expanded_url' in url]\n",
        "\n",
        "            tweets.append(tweet_data)\n",
        "\n",
        "        # Process user information\n",
        "        if response.includes and 'users' in response.includes:\n",
        "            for user in response.includes['users']:\n",
        "                user_data = {\n",
        "                    'author_id': user.id,\n",
        "                    'username': user.username,\n",
        "                    'name': user.name,\n",
        "                    'verified': user.verified,\n",
        "                    'description': user.description,\n",
        "                    'location': user.location\n",
        "                }\n",
        "                users.append(user_data)\n",
        "\n",
        "    # Create DataFrames\n",
        "    tweets_df = pd.DataFrame(tweets)\n",
        "    users_df = pd.DataFrame(users)\n",
        "\n",
        "    # Merge tweet and user data\n",
        "    full_df = pd.merge(tweets_df, users_df, on='author_id', how='left')\n",
        "\n",
        "    # Save to CSV with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"kannada_religious_tweets_{timestamp}.csv\"\n",
        "    full_df.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"Successfully collected {len(tweets)} tweets. Saved to {filename}\")\n",
        "    print(f\"Dataset columns: {full_df.columns.tolist()}\")\n",
        "\n",
        "except tweepy.TweepyException as e:\n",
        "    print(f\"Error fetching tweets: {e}\")"
      ],
      "metadata": {
        "id": "tbAN8u5C4iqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}